{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81bbf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Active Learning Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [00:03<00:00, 20.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AL 1 | Epoch 1/6] Loss: 0.9822 | Time: 1.37s\n",
      "[AL 1 | Epoch 2/6] Loss: 0.5722 | Time: 1.38s\n",
      "[AL 1 | Epoch 3/6] Loss: 0.6498 | Time: 1.39s\n",
      "[AL 1 | Epoch 4/6] Loss: 0.4468 | Time: 1.41s\n",
      "[AL 1 | Epoch 5/6] Loss: 0.4391 | Time: 1.40s\n",
      "[AL 1 | Epoch 6/6] Loss: 0.3414 | Time: 1.39s\n",
      "\n",
      "üì§ Running inference on test set after AL round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 533/533 [00:28<00:00, 18.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Class-Agnostic mAP@0.50 after AL round 1: 0.7204\n",
      "\n",
      "üîÅ Active Learning Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [00:02<00:00, 28.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AL 2 | Epoch 1/6] Loss: 1.5966 | Time: 1.71s\n",
      "[AL 2 | Epoch 2/6] Loss: 1.1208 | Time: 1.73s\n",
      "[AL 2 | Epoch 3/6] Loss: 0.9528 | Time: 1.72s\n",
      "[AL 2 | Epoch 4/6] Loss: 0.7816 | Time: 1.70s\n",
      "[AL 2 | Epoch 5/6] Loss: 0.6952 | Time: 1.74s\n",
      "[AL 2 | Epoch 6/6] Loss: 0.6557 | Time: 1.73s\n",
      "\n",
      "üì§ Running inference on test set after AL round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 533/533 [00:29<00:00, 18.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Class-Agnostic mAP@0.50 after AL round 2: 0.7935\n",
      "\n",
      "üîÅ Active Learning Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [00:02<00:00, 31.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AL 3 | Epoch 1/6] Loss: 0.6525 | Time: 2.07s\n",
      "[AL 3 | Epoch 2/6] Loss: 0.6590 | Time: 2.03s\n",
      "[AL 3 | Epoch 3/6] Loss: 0.5611 | Time: 2.04s\n",
      "[AL 3 | Epoch 4/6] Loss: 0.5164 | Time: 2.01s\n",
      "[AL 3 | Epoch 5/6] Loss: 0.4558 | Time: 2.08s\n",
      "[AL 3 | Epoch 6/6] Loss: 0.4079 | Time: 2.14s\n",
      "\n",
      "üì§ Running inference on test set after AL round 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 533/533 [00:28<00:00, 18.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Class-Agnostic mAP@0.50 after AL round 3: 0.7397\n",
      "\n",
      "üîÅ Active Learning Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [00:01<00:00, 35.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AL 4 | Epoch 1/6] Loss: 0.7298 | Time: 2.26s\n",
      "[AL 4 | Epoch 2/6] Loss: 0.6932 | Time: 2.40s\n",
      "[AL 4 | Epoch 3/6] Loss: 0.6262 | Time: 2.38s\n",
      "[AL 4 | Epoch 4/6] Loss: 0.6388 | Time: 2.53s\n",
      "[AL 4 | Epoch 5/6] Loss: 0.5021 | Time: 2.46s\n",
      "[AL 4 | Epoch 6/6] Loss: 0.4910 | Time: 2.49s\n",
      "\n",
      "üì§ Running inference on test set after AL round 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 533/533 [00:28<00:00, 18.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Class-Agnostic mAP@0.50 after AL round 4: 0.7820\n",
      "\n",
      "üîÅ Active Learning Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [00:01<00:00, 43.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AL 5 | Epoch 1/6] Loss: 0.5710 | Time: 2.65s\n",
      "[AL 5 | Epoch 2/6] Loss: 0.4591 | Time: 2.82s\n",
      "[AL 5 | Epoch 3/6] Loss: 0.3707 | Time: 2.70s\n",
      "[AL 5 | Epoch 4/6] Loss: 0.3074 | Time: 2.76s\n",
      "[AL 5 | Epoch 5/6] Loss: 0.2626 | Time: 2.70s\n",
      "[AL 5 | Epoch 6/6] Loss: 0.2279 | Time: 2.73s\n",
      "\n",
      "üì§ Running inference on test set after AL round 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 533/533 [00:28<00:00, 18.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Class-Agnostic mAP@0.50 after AL round 5: 0.8272\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.models.detection.image_list import ImageList\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Paths\n",
    "image_dir = '/home/umang.shikarvar/data/lucknow_airshed/val/images'\n",
    "label_dir = '/home/umang.shikarvar/data/lucknow_airshed/val/labels'\n",
    "save_path = '/home/umang.shikarvar/AL/AL_detectors'\n",
    "entropy_file = '/home/umang.shikarvar/AL/uncertain_images.txt'\n",
    "model_path = '/home/umang.shikarvar/AL/source_detectors/model_epoch_30.pth'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Image transform\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Dataset ===\n",
    "class TIFRCNNDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, selected_filenames, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transforms = transforms\n",
    "        self.image_filenames = sorted([\n",
    "            f for f in os.listdir(image_dir)\n",
    "            if f.endswith('.tif') and f in selected_filenames\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_filenames[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        label_path = os.path.join(self.label_dir, os.path.splitext(img_name)[0] + \".txt\")\n",
    "        boxes, labels = [], []\n",
    "\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        try:\n",
    "                            cls, x1, y1, x2, y2 = map(float, line.strip().split())\n",
    "                            boxes.append([x1, y1, x2, y2])\n",
    "                            labels.append(int(cls) + 1)  # background=0\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4))\n",
    "        labels = torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros((0,), dtype=torch.int64)\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "        image = self.transforms(image) if self.transforms else image\n",
    "\n",
    "        return image, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# === Model ===\n",
    "backbone = resnet_fpn_backbone(\n",
    "    backbone_name='resnet50',\n",
    "    weights=ResNet50_Weights.IMAGENET1K_V1\n",
    ")\n",
    "model = FasterRCNN(backbone, num_classes=4)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# === Entropy function ===\n",
    "def compute_entropy(logits):\n",
    "    probs = F.softmax(logits[:, 1:], dim=1)  # skip background\n",
    "    entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)\n",
    "    return entropy\n",
    "\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "def load_txt_as_ground_truth(txt_path):\n",
    "    boxes, labels = [], []\n",
    "    if os.path.exists(txt_path):\n",
    "        with open(txt_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    continue\n",
    "                cls, x1, y1, x2, y2 = map(float, parts)\n",
    "                labels.append(int(cls))\n",
    "                boxes.append([x1, y1, x2, y2])\n",
    "    return {\n",
    "        \"boxes\": torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4)),\n",
    "        \"labels\": torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros((0,), dtype=torch.int64)\n",
    "    }\n",
    "\n",
    "def load_txt_as_prediction(txt_path):\n",
    "    boxes, labels, scores = [], [], []\n",
    "    if os.path.exists(txt_path):\n",
    "        with open(txt_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 6:\n",
    "                    continue\n",
    "                cls, conf, x1, y1, x2, y2 = map(float, parts)\n",
    "                labels.append(int(cls))\n",
    "                scores.append(conf)\n",
    "                boxes.append([x1, y1, x2, y2])\n",
    "    return {\n",
    "        \"boxes\": torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4)),\n",
    "        \"labels\": torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros((0,), dtype=torch.int64),\n",
    "        \"scores\": torch.tensor(scores, dtype=torch.float32) if scores else torch.zeros((0,))\n",
    "    }\n",
    "\n",
    "def make_class_agnostic(detection_list):\n",
    "    for d in detection_list:\n",
    "        d[\"labels\"] = torch.zeros_like(d[\"labels\"])\n",
    "    return detection_list\n",
    "\n",
    "def compute_map(test_img_dir, test_label_dir, pred_dir):\n",
    "    image_filenames = sorted([\n",
    "        f for f in os.listdir(test_img_dir) if f.lower().endswith(\".tif\")\n",
    "    ])\n",
    "\n",
    "    gt_targets, predictions = [], []\n",
    "\n",
    "    for fname in image_filenames:\n",
    "        base = os.path.splitext(fname)[0]\n",
    "        gt = load_txt_as_ground_truth(os.path.join(test_label_dir, base + \".txt\"))\n",
    "        pred = load_txt_as_prediction(os.path.join(pred_dir, base + \".txt\"))\n",
    "        gt_targets.append(gt)\n",
    "        predictions.append(pred)\n",
    "\n",
    "    gt_targets = make_class_agnostic(gt_targets)\n",
    "    predictions = make_class_agnostic(predictions)\n",
    "\n",
    "    metric = MeanAveragePrecision()\n",
    "    metric.update(predictions, gt_targets)\n",
    "    results = metric.compute()\n",
    "    return results[\"map_50\"].item()\n",
    "\n",
    "# === Active Learning Loop ===\n",
    "cumulative_filenames = set()\n",
    "top_k = 10\n",
    "al_rounds = 5\n",
    "fine_tune_epochs = 6\n",
    "\n",
    "for al_epoch in range(al_rounds):\n",
    "    print(f\"\\nüîÅ Active Learning Epoch {al_epoch + 1}/{al_rounds}\")\n",
    "\n",
    "    if al_epoch==0:\n",
    "        top_k=20\n",
    "    else:\n",
    "        top_k=5\n",
    "\n",
    "    # 1. Evaluate entropy over all images\n",
    "    model.eval()\n",
    "    image_uncertainties = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for filename in tqdm(os.listdir(image_dir)):\n",
    "            if not filename.lower().endswith((\".jpg\", \".png\", \".tif\")):\n",
    "                continue\n",
    "\n",
    "            if filename in cumulative_filenames:\n",
    "                continue  # Skip already added images\n",
    "\n",
    "            img_path = os.path.join(image_dir, filename)\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            image_size = [tuple(image_tensor.shape[-2:])]\n",
    "\n",
    "            # Backbone feature extraction\n",
    "            features = model.backbone(image_tensor)\n",
    "            images = ImageList(image_tensor, image_size)\n",
    "            proposals, _ = model.rpn(images, features)\n",
    "\n",
    "            if len(proposals[0]) == 0:\n",
    "                image_uncertainties.append((filename, 0.0))\n",
    "                continue\n",
    "\n",
    "            box_features = model.roi_heads.box_roi_pool(features, proposals, image_size)\n",
    "            box_features = model.roi_heads.box_head(box_features)\n",
    "            class_logits = model.roi_heads.box_predictor.cls_score(box_features)\n",
    "\n",
    "            entropy = compute_entropy(class_logits)\n",
    "            avg_entropy = entropy.mean().item()\n",
    "            image_uncertainties.append((filename, avg_entropy))\n",
    "\n",
    "    # 2. Select top-k uncertain images\n",
    "    image_uncertainties.sort(key=lambda x: x[1], reverse=True)\n",
    "    new_filenames = [fname for fname, _ in image_uncertainties[:top_k]]\n",
    "    cumulative_filenames.update(new_filenames)\n",
    "\n",
    "    # Save cumulative set\n",
    "    with open(entropy_file, \"w\") as f:\n",
    "        for fname in sorted(cumulative_filenames):\n",
    "            f.write(f\"{fname}\\n\")\n",
    "\n",
    "    # 3. Build Dataset and Dataloader with updated set\n",
    "    dataset = TIFRCNNDataset(\n",
    "        image_dir=image_dir,\n",
    "        label_dir=label_dir,\n",
    "        selected_filenames=cumulative_filenames,\n",
    "        transforms=transform\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=10, shuffle=True, pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "    # 4. Fine-tune the model\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam([p for p in model.parameters() if p.requires_grad], lr=1e-4)\n",
    "\n",
    "    for epoch in range(fine_tune_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for images, targets in dataloader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += losses.item()\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"[AL {al_epoch + 1} | Epoch {epoch + 1}/{fine_tune_epochs}] Loss: {epoch_loss:.4f} | Time: {end_time - start_time:.2f}s\")\n",
    "\n",
    "    # Save model after each AL round\n",
    "    torch.save(model.state_dict(), os.path.join(save_path, f\"model_al_round_{al_epoch + 1}.pth\"))\n",
    "\n",
    "    # === Step 5: Run inference on test set after fine-tuning ===\n",
    "    print(f\"\\nüì§ Running inference on test set after AL round {al_epoch + 1}\")\n",
    "\n",
    "    test_image_dir = \"/home/umang.shikarvar/data/lucknow_airshed/test/images\"\n",
    "    prediction_output_dir = \"/home/umang.shikarvar/data/lucknow_predictions\"\n",
    "    \n",
    "    # Clear previous predictions\n",
    "    if os.path.exists(prediction_output_dir):\n",
    "        for f in os.listdir(prediction_output_dir):\n",
    "            os.remove(os.path.join(prediction_output_dir, f))\n",
    "    else:\n",
    "        os.makedirs(prediction_output_dir)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for img_file in tqdm(os.listdir(test_image_dir)):\n",
    "            if not img_file.lower().endswith(\".tif\"):\n",
    "                continue\n",
    "\n",
    "            image_path = os.path.join(test_image_dir, img_file)\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            outputs = model(image_tensor)[0]  # Get single-image output\n",
    "\n",
    "            base_name = os.path.splitext(img_file)[0]\n",
    "            pred_txt_path = os.path.join(prediction_output_dir, base_name + \".txt\")\n",
    "\n",
    "            with open(pred_txt_path, \"w\") as f:\n",
    "                for box, label, score in zip(outputs[\"boxes\"], outputs[\"labels\"], outputs[\"scores\"]):\n",
    "                    x1, y1, x2, y2 = box.tolist()\n",
    "                    f.write(f\"{label.item()} {score.item():.4f} {x1:.1f} {y1:.1f} {x2:.1f} {y2:.1f}\\n\")\n",
    "\n",
    "    map50 = compute_map(\n",
    "    test_img_dir=\"/home/umang.shikarvar/data/lucknow_airshed/test/images\",\n",
    "    test_label_dir=\"/home/umang.shikarvar/data/lucknow_airshed/test/labels\",\n",
    "    pred_dir=\"/home/umang.shikarvar/AL/lucknow_predictions\"\n",
    ")\n",
    "\n",
    "    print(f\"‚úÖ Class-Agnostic mAP@0.50 after AL round {al_epoch + 1}: {map50:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
