{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e27609a2",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb2e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df51a67",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63f0c82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ImageDataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.file_paths = [\n",
    "            os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path)\n",
    "        ]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.file_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Open the image and convert to RGB\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply transformations if provided\n",
    "        return image\n",
    "\n",
    "# Define transformation pipeline\n",
    "transform_pipeline = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL Image to Tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "def load_images(folder_path):\n",
    "    dataset = ImageDataset(folder_path, transform=transform_pipeline)\n",
    "    # Added num_workers and pin_memory for better performance\n",
    "    return DataLoader(\n",
    "        dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=4,  # Adjust this based on your CPU cores\n",
    "        pin_memory=True  # Speeds up data transfer to GPU if using CUDA\n",
    "    )\n",
    "\n",
    "# Load datasets\n",
    "source = load_images('/home/umang.shikarvar/instaformer/wb_small_airshed/images')  # Give source path\n",
    "target = load_images('/home/umang.shikarvar/instaformer/delhi_ncr_small/images')   # Give target path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6f403a",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43569e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.pool(x), x  # (pooled output, pre-pool features)\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, 2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(out_ch * 2, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([x, skip], dim=1) # Concatenate with skip connection\n",
    "        return self.conv(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = EncoderBlock(3, 64)    # 640x640x3→320x320x64\n",
    "        self.enc2 = EncoderBlock(64, 128)  # 320x320x64→160x160x128\n",
    "        self.enc3 = EncoderBlock(128, 256) # 160x160x128→80x80x256\n",
    "        self.enc4 = EncoderBlock(256, 512) # 80x80x256→40x40x512\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = Bottleneck(512, 1024) # 40x40x512→40x40x1024\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec4 = DecoderBlock(1024, 512) # 40x40x1024→80x80x512\n",
    "        self.dec3 = DecoderBlock(512, 256) # 80x80x512→160x160x256\n",
    "        self.dec2 = DecoderBlock(256, 128) # 160x160x256→320x320x128\n",
    "        self.dec1 = DecoderBlock(128, 64) # 320x320x128→640x640x64\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(64, 3, 1), # 640x640x64→640x640x3\n",
    "            nn.Tanh() # Normalize to [-1, 1]\n",
    "        )\n",
    "\n",
    "    def encoder(self, x):\n",
    "        # Encoder forward pass only\n",
    "        x, s1 = self.enc1(x)\n",
    "        x, s2 = self.enc2(x)\n",
    "        x, s3 = self.enc3(x)\n",
    "        x, s4 = self.enc4(x)\n",
    "        return [s1, s2, s3, s4]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder with skip connections\n",
    "        x, s1 = self.enc1(x)  # x: 320x320x64, s1: 640x640x64\n",
    "        x, s2 = self.enc2(x)  # x: 160x160x128, s2: 320x320x128\n",
    "        x, s3 = self.enc3(x)  # x: 80x80x256, s3: 160x160x256\n",
    "        x, s4 = self.enc4(x)  # x: 40x40x512, s4: 80x80x512\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)  # 40x40x1024\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        x = self.dec4(x, s4)  # 80x80x512 using x: 40x40x1024, s4: 80x80x512\n",
    "        x = self.dec3(x, s3)  # 160x160x256 using x: 80x80x512, s3: 160x160x256\n",
    "        x = self.dec2(x, s2)  # 320x320x128 using x: 160x160x256, s2: 320x320x128\n",
    "        x = self.dec1(x, s1)  # 640x640x64 using x: 320x320x128, s1: 640x640x64\n",
    "        \n",
    "        return self.out(x), [s1, s2, s3, s4]\n",
    "\n",
    "class HEncoder(nn.Module):  \n",
    "    def __init__(self, input_channels, output_dim=300):\n",
    "        super().__init__()\n",
    "        # Layer-specific MLPs\n",
    "        self.proj = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(C, output_dim),  # Channel-wise transformation (C → 256)\n",
    "                nn.ReLU()\n",
    "            ) for C in input_channels\n",
    "        ])\n",
    "        \n",
    "    def forward(self, features):\n",
    "        embeddings = []\n",
    "        for i, (proj, f) in enumerate(zip(self.proj, features)):\n",
    "            \n",
    "            # Reshape and apply MLP\n",
    "            B, C, H, W = f.shape  # Update after downsampling\n",
    "            f = f.permute(0, 2, 3, 1).reshape(B, H * W, C)  # [B, S, C]\n",
    "            f_projected = proj(f)  # Apply MLP to each patch → [B, S, D]\n",
    "            \n",
    "            embeddings.append(f_projected)\n",
    "        return embeddings\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1),  # 640x640x3→320x320x64\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),  # 320x320x64→160x160x128\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),  # 160x160x128→80x80x256\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),  # 80x80x256→40x40x512\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(512, 1, 4, 1, 1)  # 40x40x512→39x39x1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c076bb74",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "532caf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NCELoss(f_q, f_k, patch_count=4096, tau=0.07):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        f_q, f_k: (B, S, D) - patch features per image\n",
    "    Returns:\n",
    "        InfoNCE loss using N patches per image\n",
    "    \"\"\"\n",
    "    B, S, D = f_q.shape\n",
    "    N = min(patch_count, S)  # N patches per image\n",
    "\n",
    "    # Normalize feature vectors\n",
    "    f_q = F.normalize(f_q, dim=2)  # (B, S, D)\n",
    "    f_k = F.normalize(f_k, dim=2)  # (B, S, D)\n",
    "\n",
    "    # Sample N patch indices per image\n",
    "    idx = torch.randint(0, S, (B, N), device=f_q.device)  # (B, N)\n",
    "\n",
    "    # Gather patches using batched indexing\n",
    "    batch_indices = torch.arange(B, device=f_q.device).unsqueeze(1).expand(B, N)  # (B, N)\n",
    "    f_q_sampled = f_q[batch_indices, idx]  # (B, N, D)\n",
    "    f_k_sampled = f_k[batch_indices, idx]  # (B, N, D)\n",
    "\n",
    "    # Compute positive logits: dot product of corresponding patches\n",
    "    l_pos = torch.sum(f_q_sampled * f_k_sampled, dim=2, keepdim=True)  # (B, N, 1)\n",
    "\n",
    "    # Compute negative logits: all other patches in the same image\n",
    "    logits_neg = torch.bmm(f_q_sampled, f_k_sampled.transpose(1, 2))  # (B, N, N)\n",
    "    mask = torch.eye(N, device=f_q.device).unsqueeze(0).bool()  # (1, N, N)\n",
    "    logits_neg = logits_neg.masked_fill(mask, -1e9)\n",
    "\n",
    "    # Combine and compute loss\n",
    "    logits = torch.cat([l_pos, logits_neg], dim=2) / tau  # (B, N, 1+N)\n",
    "    labels = torch.zeros(B, N, dtype=torch.long, device=f_q.device)\n",
    "\n",
    "    return F.cross_entropy(logits.reshape(-1, 1 + N), labels.reshape(-1))\n",
    "\n",
    "def contrastive_loss(embeddings_x, embeddings_gx, nce_loss_fn):\n",
    "    \"\"\"\n",
    "    Compute multilayer contrastive loss\n",
    "\n",
    "    Args:\n",
    "        embeddings_x: List of input features from H(G_enc(x))\n",
    "        embeddings_gx: List of output features from H(G_enc(G(x)))\n",
    "        nce_loss_fn: A function like NCELoss\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    num_layers = len(embeddings_x)\n",
    "    \n",
    "    for f_x, f_gx in zip(embeddings_x, embeddings_gx):\n",
    "        total_loss += nce_loss_fn(f_gx, f_x)  # Query=G(x), Key=x\n",
    "    \n",
    "    return total_loss / num_layers\n",
    "\n",
    "def adversarial_loss(D, real_images, fake_images):\n",
    "    \"\"\" \n",
    "    LSGAN Adversarial Loss.\n",
    "    Args:\n",
    "        D: Discriminator model\n",
    "        real_images: Real images from dataset\n",
    "        fake_images: Generated images from Generator\n",
    "        \n",
    "    Returns:\n",
    "        Discriminator loss, Generator loss\n",
    "    \"\"\"\n",
    "    real_preds = D(real_images)  # Real image predictions\n",
    "    fake_preds = D(fake_images.detach())  # Fake image predictions\n",
    "\n",
    "    # LSGAN losses\n",
    "    d_real_loss = 0.5 * torch.mean((real_preds - 1) ** 2)\n",
    "    d_fake_loss = 0.5 * torch.mean(fake_preds ** 2)\n",
    "    d_loss = d_real_loss + d_fake_loss\n",
    "\n",
    "    g_loss = 0.5 * torch.mean((D(fake_images) - 1) ** 2)\n",
    "\n",
    "    return d_loss, g_loss\n",
    "\n",
    "def identity_loss(G, real_target_images):\n",
    "    \"\"\"\n",
    "    Identity loss: Ensures G preserves content when given target images.\n",
    "    \n",
    "    Args:\n",
    "        G: Generator model\n",
    "        real_target_images: Real images from target domain\n",
    "        \n",
    "    Returns:\n",
    "        Identity loss\n",
    "    \"\"\"\n",
    "    identity_output = G(real_target_images)[0]  # Generator output\n",
    "    return torch.mean(torch.abs(identity_output - real_target_images))  # L1 Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73096b0e",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e4b2e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "EPOCHS = 200\n",
    "\n",
    "# Set device to GPU or CPU\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize models\n",
    "G = Generator().to(device)\n",
    "H = HEncoder([64, 128, 256, 512]).to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "# Define optimizers\n",
    "g_optimizer = optim.Adam(G.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "h_optimizer = optim.Adam(H.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(D.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "# Learning rate schedulers\n",
    "g_scheduler = optim.lr_scheduler.StepLR(g_optimizer, step_size=20, gamma=0.5)\n",
    "h_scheduler = optim.lr_scheduler.StepLR(h_optimizer, step_size=20, gamma=0.5)\n",
    "d_scheduler = optim.lr_scheduler.StepLR(d_optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "# Hyperparameters\n",
    "lambda_Y = 1  # Scaling factor for identity loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5973627d",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aed2b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorBoard writer\n",
    "log_dir = \"/home/umang.shikarvar/instaformer/CUT_logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    total_d_loss = 0.0\n",
    "    total_g_loss = 0.0\n",
    "    total_contrastive_loss = 0.0\n",
    "    total_identity_loss = 0.0\n",
    "\n",
    "    for real_x, real_y in zip(source, target):\n",
    "        real_x, real_y = real_x.to(device), real_y.to(device)\n",
    "\n",
    "        # ==============================\n",
    "        # 1. Train Discriminator\n",
    "        # ==============================\n",
    "        G_x, _ = G(real_x)  # Forward through generator\n",
    "\n",
    "        d_loss, _ = adversarial_loss(D, real_y, G_x.detach())  # Detach to avoid backward through G\n",
    "\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # ==============================\n",
    "        # 2. Train Generator & Projection Head\n",
    "        # ==============================\n",
    "        G_x, features_x = G(real_x)  # Forward through generator again\n",
    "        features_G_x = G.encoder(G_x)  # Encode G(x)\n",
    "\n",
    "        # Project features using H (projection head)\n",
    "        embeddings_x = H(features_x)\n",
    "        embeddings_gx = H(features_G_x)\n",
    "\n",
    "        contrastive_loss_val = contrastive_loss(embeddings_x, embeddings_gx, NCELoss)\n",
    "\n",
    "        _, g_loss = adversarial_loss(D, real_y, G_x)  # Generator adversarial loss\n",
    "        identity_loss_val = identity_loss(G, real_y)\n",
    "\n",
    "        total_loss = g_loss + contrastive_loss_val + lambda_Y * identity_loss_val\n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "        h_optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        h_optimizer.step()\n",
    "\n",
    "        # Accumulate losses\n",
    "        total_d_loss += d_loss.item()\n",
    "        total_g_loss += g_loss.item()\n",
    "        total_contrastive_loss += contrastive_loss_val.item()\n",
    "        total_identity_loss += identity_loss_val.item()\n",
    "\n",
    "    # Scheduler steps\n",
    "    d_scheduler.step()\n",
    "    g_scheduler.step()\n",
    "    h_scheduler.step()\n",
    "\n",
    "    # TensorBoard logging\n",
    "    writer.add_scalar(\"Loss/Discriminator\", total_d_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/Generator\", total_g_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/Contrastive\", total_contrastive_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/Identity\", total_identity_loss, epoch)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - D Loss: {total_d_loss:.4f}, \"\n",
    "          f\"G Loss: {total_g_loss:.4f}, \"\n",
    "          f\"Contrastive Loss: {total_contrastive_loss:.4f}, \"\n",
    "          f\"Identity Loss: {total_identity_loss:.4f}\")      \n",
    "\n",
    "    # Checkpoint\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save(G.state_dict(), f\"/home/umang.shikarvar/instaformer/CUT_gen/generator_CUT_{epoch+1}.pth\")\n",
    "\n",
    "writer.close()\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
