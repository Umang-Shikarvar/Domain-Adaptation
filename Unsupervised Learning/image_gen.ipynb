{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU or CPU\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# Define the ImageDataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_paths = [os.path.join(image_dir, img) for img in os.listdir(image_dir)]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, img_path  # Return both image and path\n",
    "\n",
    "# Define transformation pipeline\n",
    "transform_pipeline = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL Image to Tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "def load_images(folder_path):\n",
    "    dataset = ImageDataset(folder_path, transform=transform_pipeline)\n",
    "    # Added num_workers and pin_memory for better performance\n",
    "    return DataLoader(\n",
    "        dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=4,  # Adjust this based on your CPU cores\n",
    "        pin_memory=True  # Speeds up data transfer to GPU if using CUDA\n",
    "    )\n",
    "\n",
    "# Load datasets\n",
    "source = load_images('/home/umang.shikarvar/instaformer/wb_small_airshed/images')  # Give source path\n",
    "target = load_images('/home/umang.shikarvar/instaformer/delhi_ncr_small/images')   # Give target path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "class ConvolutionalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, is_downsampling=True, add_activation=True, **kwargs):\n",
    "        super().__init__()\n",
    "        if is_downsampling:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, padding_mode=\"reflect\", **kwargs),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True) if add_activation else nn.Identity(),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, **kwargs),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True) if add_activation else nn.Identity(),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ConvolutionalBlock(channels, channels, add_activation=True, kernel_size=3, padding=1),\n",
    "            ConvolutionalBlock(channels, channels, add_activation=False, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_channels, num_features=64, num_residuals=6):\n",
    "        super().__init__()\n",
    "        self.initial_layer = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, num_features, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\"),\n",
    "            nn.InstanceNorm2d(num_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.downsampling_layers = nn.ModuleList([\n",
    "            ConvolutionalBlock(num_features, num_features * 2, is_downsampling=True, kernel_size=3, stride=2, padding=1),\n",
    "            ConvolutionalBlock(num_features * 2, num_features * 4, is_downsampling=True, kernel_size=3, stride=2, padding=1),\n",
    "        ])\n",
    "        self.residual_layers = nn.Sequential(*[ResidualBlock(num_features * 4) for _ in range(num_residuals)])\n",
    "        self.upsampling_layers = nn.ModuleList([\n",
    "            ConvolutionalBlock(num_features * 4, num_features * 2, is_downsampling=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            ConvolutionalBlock(num_features * 2, num_features * 1, is_downsampling=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        ])\n",
    "        self.last_layer = nn.Conv2d(num_features, img_channels, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_layer(x)\n",
    "        for layer in self.downsampling_layers:\n",
    "            x = layer(x)\n",
    "        x = self.residual_layers(x)\n",
    "        for layer in self.upsampling_layers:\n",
    "            x = layer(x)\n",
    "        return torch.tanh(self.last_layer(x))\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        self.initial_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features[0], kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\"),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        layers = []\n",
    "        in_channels = features[0]\n",
    "        for feature in features[1:]:\n",
    "            layers.append(\n",
    "                nn.Conv2d(in_channels, feature, kernel_size=4, stride=2 if feature != features[-1] else 1, padding=1, padding_mode=\"reflect\"),\n",
    "            )\n",
    "            layers.append(nn.InstanceNorm2d(feature))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            in_channels = feature\n",
    "        layers.append(nn.Conv2d(in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_layer(x)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator model (assuming it's already defined in your environment)\n",
    "generator_g = Generator(img_channels=3).to(device)\n",
    "\n",
    "# Load pre-trained weights\n",
    "generator_g.load_state_dict(torch.load('/home/umang.shikarvar/instaformer/wb_CG_gen/generator_CG_200.pth', map_location=device)) # path to model\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "generator_g.eval()\n",
    "\n",
    "# Function to denormalize images (assuming they were normalized to [-1, 1])\n",
    "def denormalize(tensor):\n",
    "    return (tensor * 0.5) + 0.5\n",
    "\n",
    "# Function to save generated images\n",
    "def save_image(tensor, path):\n",
    "    \"\"\"Saves a PyTorch tensor as an image file.\"\"\"\n",
    "    tensor = denormalize(tensor).clamp(0, 1)  # Denormalize and clamp values\n",
    "    vutils.save_image(tensor, path)\n",
    "\n",
    "# Function to generate and save images with original file names\n",
    "def generate_and_save_images(generator, dataloader, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create output directory if not exists\n",
    "\n",
    "    for example_input, img_path in dataloader:  # Now returns both image and path\n",
    "        example_input = example_input.to(device)  # Move to device\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_image = generator(example_input)  # Generate image\n",
    "\n",
    "        # Extract original filename without extension\n",
    "        original_filename = os.path.basename(img_path[0])  # Get first item from batch\n",
    "        filename_without_ext = os.path.splitext(original_filename)[0]\n",
    "\n",
    "        # Save generated image with modified filename\n",
    "        save_path = os.path.join(output_dir, f\"{filename_without_ext}.png\")\n",
    "        save_image(generated_image, save_path)\n",
    "        print(f\"Saved: {save_path}\")\n",
    "\n",
    "# Define output directory\n",
    "output_dir = \"/home/umang.shikarvar/instaformer/wb_CG/images\"\n",
    "\n",
    "# Generate and save images\n",
    "print(\"Generating and saving images...\")\n",
    "generate_and_save_images(generator_g, source , output_dir)\n",
    "print(\"Image generation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.pool(x), x  # (pooled output, pre-pool features)\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, 2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(out_ch * 2, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([x, skip], dim=1) # Concatenate with skip connection\n",
    "        return self.conv(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = EncoderBlock(3, 64)    # 640x640x3→320x320x64\n",
    "        self.enc2 = EncoderBlock(64, 128)  # 320x320x64→160x160x128\n",
    "        self.enc3 = EncoderBlock(128, 256) # 160x160x128→80x80x256\n",
    "        self.enc4 = EncoderBlock(256, 512) # 80x80x256→40x40x512\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = Bottleneck(512, 1024) # 40x40x512→40x40x1024\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec4 = DecoderBlock(1024, 512) # 40x40x1024→80x80x512\n",
    "        self.dec3 = DecoderBlock(512, 256) # 80x80x512→160x160x256\n",
    "        self.dec2 = DecoderBlock(256, 128) # 160x160x256→320x320x128\n",
    "        self.dec1 = DecoderBlock(128, 64) # 320x320x128→640x640x64\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(64, 3, 1), # 640x640x64→640x640x3\n",
    "            nn.Tanh() # Normalize to [-1, 1]\n",
    "        )\n",
    "\n",
    "    def encoder(self, x):\n",
    "        # Encoder forward pass only\n",
    "        x, s1 = self.enc1(x)\n",
    "        x, s2 = self.enc2(x)\n",
    "        x, s3 = self.enc3(x)\n",
    "        x, s4 = self.enc4(x)\n",
    "        return [s1, s2, s3, s4]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder with skip connections\n",
    "        x, s1 = self.enc1(x)  # x: 320x320x64, s1: 640x640x64\n",
    "        x, s2 = self.enc2(x)  # x: 160x160x128, s2: 320x320x128\n",
    "        x, s3 = self.enc3(x)  # x: 80x80x256, s3: 160x160x256\n",
    "        x, s4 = self.enc4(x)  # x: 40x40x512, s4: 80x80x512\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)  # 40x40x1024\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        x = self.dec4(x, s4)  # 80x80x512 using x: 40x40x1024, s4: 80x80x512\n",
    "        x = self.dec3(x, s3)  # 160x160x256 using x: 80x80x512, s3: 160x160x256\n",
    "        x = self.dec2(x, s2)  # 320x320x128 using x: 160x160x256, s2: 320x320x128\n",
    "        x = self.dec1(x, s1)  # 640x640x64 using x: 320x320x128, s1: 640x640x64\n",
    "        \n",
    "        return self.out(x), [s1, s2, s3, s4]\n",
    "\n",
    "class HEncoder(nn.Module):  \n",
    "    def __init__(self, input_channels, output_dim=256):\n",
    "        super().__init__()\n",
    "        # Layer-specific MLPs\n",
    "        self.proj = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(C, output_dim),  # Channel-wise transformation (C → 256)\n",
    "                nn.ReLU()\n",
    "            ) for C in input_channels\n",
    "        ])\n",
    "        \n",
    "    def forward(self, features):\n",
    "        embeddings = []\n",
    "        for i, (proj, f) in enumerate(zip(self.proj, features)):\n",
    "            \n",
    "            # Reshape and apply MLP\n",
    "            B, C, H, W = f.shape  # Update after downsampling\n",
    "            f = f.permute(0, 2, 3, 1).reshape(B, H * W, C)  # [B, S, C]\n",
    "            f_projected = proj(f)  # Apply MLP to each patch → [B, S, D]\n",
    "            \n",
    "            embeddings.append(f_projected)\n",
    "        return embeddings\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1),  # 640x640x3→320x320x64\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),  # 320x320x64→160x160x128\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),  # 160x160x128→80x80x256\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),  # 80x80x256→40x40x512\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(512, 1, 4, 1, 1)  # 40x40x512→39x39x1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator model (assuming it's already defined in your environment)\n",
    "generator_g = Generator().to(device)  # Move model to device\n",
    "\n",
    "# Load the trained model parameters\n",
    "model_path = \"/home/umang.shikarvar/instaformer/CUT_gen/generator_CUT_200.pth\" # path to model\n",
    "generator_g.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "generator_g.eval()\n",
    "\n",
    "# Function to denormalize images (assuming they were normalized to [-1, 1])\n",
    "def denormalize(tensor):\n",
    "    return (tensor * 0.5) + 0.5\n",
    "\n",
    "# Function to save generated images\n",
    "def save_image(tensor, path):\n",
    "    \"\"\"Saves a PyTorch tensor as an image file.\"\"\"\n",
    "    tensor = denormalize(tensor).clamp(0, 1)  # Denormalize and clamp values\n",
    "    vutils.save_image(tensor, path)\n",
    "\n",
    "# Function to generate and save images with original file names\n",
    "def generate_and_save_images(generator, dataloader, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create output directory if not exists\n",
    "\n",
    "    for example_input, img_path in dataloader:  # Now returns both image and path\n",
    "        example_input = example_input.to(device)  # Move to device\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_image,_ = generator(example_input)  # Generate image\n",
    "\n",
    "        # Extract original filename without extension\n",
    "        original_filename = os.path.basename(img_path[0])  # Get first item from batch\n",
    "        filename_without_ext = os.path.splitext(original_filename)[0]\n",
    "\n",
    "        # Save generated image with modified filename\n",
    "        save_path = os.path.join(output_dir, f\"{filename_without_ext}.png\")\n",
    "        save_image(generated_image, save_path)\n",
    "        print(f\"Saved: {save_path}\")\n",
    "\n",
    "# Define output directory\n",
    "output_dir = \"/home/umang.shikarvar/instaformer/delhi_CUT/images\"\n",
    "\n",
    "# Generate and save images\n",
    "print(\"Generating and saving images...\")\n",
    "generate_and_save_images(generator_g, source , output_dir)\n",
    "print(\"Image generation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
